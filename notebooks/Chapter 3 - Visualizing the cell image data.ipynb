{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05a0882d-e5ed-45e4-9a85-1dc34e6f775d",
   "metadata": {},
   "source": [
    "# 3 Visualization the cell image data\n",
    "**Estimated time to run through notebook is 20 minutes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754929f1-121d-4252-98d0-7ccbe767add1",
   "metadata": {},
   "source": [
    "This notebook is split into the following sections\n",
    "\n",
    "- [Installing and loading libraries](#preprocessing)\n",
    "- [3.1 Creating Single Cell Images](#creating)\n",
    "- [3.2 Visualizing Single Cell Images](#visualizing)\n",
    "- [Conclusion](#end)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48c1c67-88f6-4d3c-b769-eee58f0358e5",
   "metadata": {},
   "source": [
    "#### Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c84c825-ca6a-46f9-807d-5adbaea7fd81",
   "metadata": {},
   "source": [
    "Libraries we will be using here  - \n",
    "\n",
    "- pandas for reading and slicing manifests - https://pandas.pydata.org\n",
    "- matlpotlib for plotting - https://matplotlib.org\n",
    "- aicsimageio for image reading - https://github.com/AllenCellModeling/aicsimageio\n",
    "- nbvv for interactive 3D volume rendering - https://github.com/allen-cell-animated/nbvv\n",
    "- Cell Feature explorer for interactive image and feature exploration - https://github.com/allen-cell-animated/cell-feature-explorer\n",
    "- aicsimageprocessing for image processing and visualization utility functions - https://github.com/AllenCellModeling/aicsimageprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8492028e-9684-4e7a-8774-1585fe85e3c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <a id='preprocessing'></a>Installing and loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd395e-1a78-4f97-b2ad-0a53e729a911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from aicsimageprocessing import diagnostic_sheet, read_ome_zarr, rescale_image, imgtoprojection\n",
    "from upath import UPath as Path\n",
    "import warnings\n",
    "from aicsimageio import transforms, AICSImage\n",
    "import nbvv\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c650dd4-0299-4129-87aa-e99d34d85ed0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <a id='creating'></a> 3.1 Creating single cell images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30df1d24-322f-44f8-ae48-4cb636bcd954",
   "metadata": {},
   "source": [
    "Single cell images were created by first processing FOVs (Fields of Views), followed by additional single cell image preprocessing. We already discussed some of the FOV processing in Chapter 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f3ee67-ed66-49e3-99d2-46b928316456",
   "metadata": {},
   "source": [
    "First, lets load up the manifest and sample a cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56068c33-0d7b-4fa3-8476-fff66c559766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6ceda-4b03-4b9e-93ef-b8c60cd7c0e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample a cell\n",
    "df = pd.read_parquet(\"s3://allencell-hipsc-cytodata/hackathon_manifest_17oct2022.parquet\")\n",
    "print(f'Number of cells: {len(df)}')\n",
    "print(f'Number of columns: {len(df.columns)}')\n",
    "\n",
    "some_cell = df.sample(1).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b53d0-7965-4f28-bda3-d663a43b524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the structure, gene and protein name for this cell\n",
    "some_cell[['Structure','gene','Protein']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09593a25-b405-4e8c-9dbf-454c840c1480",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Single cell image preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400c0bdd-caaa-4ebb-925b-e08dd8e17eee",
   "metadata": {},
   "source": [
    "For the sake of the challenge, single cell images were further preprocessed. These steps include\n",
    "\n",
    "- merging in the cropped brightfield channel \n",
    "- centering and aligning the image to the longest axis of the membrane segmentation channel\n",
    "- masking by the membrane segmentation channel\n",
    "- cropping and resizing to a global 3D bounding box by taking the 95% quantile of the individual bounding box widths in x, y, and z.  \n",
    "- min-max normalization after clipping intensity values to 95% quantile values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23035f3e-6372-46a1-8bd0-fad2ebe42621",
   "metadata": {},
   "source": [
    "The final registered 3D images were saved as OME Zarr files (paths available via column **3d_image**) and will be the main input images for the challenge. Single cell image preprocessing code associated with the challenge is available at https://github.com/AllenCellModeling/Variance_Data_Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa8401f-0453-46dd-a974-45e6576f8604",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Final images saved under column name \"3d_image\"\n",
    "full_img = read_ome_zarr(some_cell[\"3d_image\"], level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9572b781-d748-4b56-afa7-7149bdf50396",
   "metadata": {},
   "source": [
    "The columns **fits_z**, **fits_x**, **fits_y**, indicate whether the image fits the bounding box in that direction. Images that fit the bounding box were padded with 0s, whereas the images that dont were cropped to that size (some information will be lost here, which may be relevant to the challenge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d26d8d4-53cd-4300-a807-30dc7d8bb420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect whether this image fits the bounding box in z, y, and x\n",
    "print(some_cell[[i for i in some_cell.index if \"fit\" in i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296156de-4cc7-46b5-8fca-f60db911a496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images have 7 channels for brightfield + DNA, membrane, and structure intensity and segmentation channels\n",
    "full_img.channel_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa455a7f-e5b8-4a92-beeb-3026acce1a86",
   "metadata": {},
   "source": [
    "Here, we see that images have 7 channels. They are \n",
    "\n",
    "- bf - brightfield channel \n",
    "- dna - dna intensity channel\n",
    "- membrane - membrane intensity channel\n",
    "- structure - structure intensity channel\n",
    "- dna_segmentation - segmentation of the dna channel\n",
    "- membrane_segmentation - segmentation of the membrane channel\n",
    "- struct_segmentation_roof - segmentation of the structure channel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539635bc-12ac-49c4-8792-4efb2f19d7af",
   "metadata": {},
   "source": [
    "After squeezing, each image is of shape CZYX - where C is the channel, and Z, Y and X are the spatial dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1eb0f1-39e7-401b-832b-dde9c574b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets look at image shape (CZYX)\n",
    "img_data = full_img.data.squeeze()\n",
    "print(img_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2c034a-e81e-4fee-8f17-d7709ba60dcd",
   "metadata": {},
   "source": [
    "All images are now of the same size, lending itself to machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599bcebe-8749-4a0a-8d40-e0332062146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_cell_1 = df.sample(1).iloc[0]\n",
    "img_data_1 = read_ome_zarr(some_cell_1[\"3d_image\"]).data.squeeze()\n",
    "\n",
    "some_cell_2 = df.sample(1).iloc[0]\n",
    "img_data_2 = read_ome_zarr(some_cell_2[\"3d_image\"]).data.squeeze()\n",
    "print('Image 1 shape', img_data_1.shape, 'Image 2 shape', img_data_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e38d2f-838d-4db3-98c0-f424e887c2a8",
   "metadata": {},
   "source": [
    "Importantly, the intensity channels in the final registered images were min-max normalized but stored with values between 0 and MX_UINT16 (65535). We can rescale the image such that the background voxels (outside the membrane segmentation mask) become -1 and the remaining voxels become min-max scaled (between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebfaa4e-ee8d-4b7d-a816-a16568f684fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"default\")\n",
    "%matplotlib inline\n",
    "fig, (ax, ax1) = plt.subplots(1,2, figsize = (10,5))\n",
    "channel_data = img_data[1]\n",
    "ax.hist(channel_data.flatten())\n",
    "ax.set_title('Original')\n",
    "\n",
    "img_data = rescale_image(img_data, full_img.channel_names)\n",
    "channel_data_rescaled = img_data[1]\n",
    "ax1.hist(channel_data_rescaled.flatten())\n",
    "ax1.set_title('Rescaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f616b4-4dfd-4027-8c0d-62031ac2230a",
   "metadata": {},
   "source": [
    "We can max project along the Z dimension and see that the pixels outside the memnbrane mask have a value of -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef3b9c6-b29f-4b9c-a69d-3cd8ecd2b837",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(channel_data_rescaled.max(0),  vmin=-1, vmax=1, aspect='auto')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55993b75-4509-4c5b-af1c-bdb76a93ef45",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <a id='visualizing'></a>3.2 Visualizing single cell images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a400c278-0b7c-448a-b9c5-cbe08085f07f",
   "metadata": {},
   "source": [
    "Single cell images can be visualized in many ways. Here we show 4 ways - using the Cell Feature Explorer (CFE), nbvv, matplotlib, and using diagnostic sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e293c0d-c3c5-4e11-9209-93fc13a68d52",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using the Cell Feature Explorer to inspect data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d06e776-22be-410f-b41a-05c4ac0f37f3",
   "metadata": {},
   "source": [
    "Cell Feature Explorer (https://cfe.allencell.org/?dataset=aics_hipsc_v2021.1) allows you to interactively examine and explore a plot of the full data set with several selected numeric features. We can also directly visualize the original unregistered single cell images and their segmentations using the standalone 3d viewer from Cell Feature Explorer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203b216-d22c-4f0c-8ebd-357f511b2fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate CFE URL\n",
    "def generate_url(cell_id):\n",
    "    print(f\"Click this link https://allen-cell-animated.github.io/website-3d-cell-viewer/?dataset=aics_hipsc_v2021.1&id={cell_id}\")\n",
    "cell_id = some_cell['CellId'].item() \n",
    "cell_id = 311375\n",
    "generate_url(cell_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b5a87-f5fb-4f69-921f-b6c3cc36c85a",
   "metadata": {},
   "source": [
    "Here, we can select *Full Field* at the top middle of the UI to switch to the FOV view."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373dff30-059c-43ae-9e60-3408edacb83e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using nbvv to inspect registered images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99027f63-8d1d-431c-9b68-bc5a19ffee53",
   "metadata": {},
   "source": [
    "Normalized 3D images can be viewed using the 3D Volume Viewer **nbvv**, a multichannel viewer for interactively examining volume data.\n",
    "\n",
    "We display our image with the volshow command. volshow accepts either a 3d or 4d data array. If the array is 4D, it will be treated as multi-channel. We can also tell volshow the physical spacing between the volume pixels in X, Y and Z dimensions (spacing). In our case the data has already been re-scaled to have equal-sized x, y, and z pixels so we just pass in [1,1,1] (the defaults) as an example. We also provide the names of the channels (channel_names) for display. \n",
    "\n",
    "Here are some of the main features you will use in the user interface:\n",
    "\n",
    "- 3D / XY / XZ / YZ Click these buttons to switch between full 3D view and side projections.\n",
    "\n",
    "- In the side projection modes, you will see one cross-sectional \"slice\" through the volume at a time.\n",
    "\n",
    "- You can change the sections you are viewing using the \"Clipping\" tab at the bottom of the viewport.\n",
    "\n",
    "- For multi-channel (4D CZYX) data, you can switch displayed channels on and off by clicking the volume checkbox next to each channel. You can also change the colors for each channel by clicking a swatch to the left. Note that the left panel is scrolling, and if you have many channels you may need to scroll. You can do contrast adjustment using the Settings (gear wheel) button for each channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9864d3-9c5f-486d-a3b3-a6e1b2fd69bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbvv.volshow(\n",
    "    img_data,\n",
    "    spacing=[1,1,1],  # full_img.physical_pixel_sizes,\n",
    "    channel_names=full_img.channel_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a47a838-cf0a-4f9b-8328-082e91468fd6",
   "metadata": {},
   "source": [
    "The viewer has automatically tried to apply some contrast adjustment, but we could easily pre-apply contrast adjustment by doing math on the data array before passing it to the viewer.  \n",
    "\n",
    "The viewer will accept any scalar-valued 3d or 4d array we give it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764f8a2a-73cb-4b20-8f2b-46c6e28baddf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using matplotlib to visualize image projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5da58d-e1fd-4e64-a646-1f1188aa05a8",
   "metadata": {},
   "source": [
    "Image projections can be viewed either by manually projecting along an axis of a single channel, or by loading up one of the projections that have been pre-computed and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6ced4b-fa56-4c0c-88ec-ce0d8355dd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually project a single channel image along a specific axis\n",
    "# Z projection of the membrane segmentation channel\n",
    "\n",
    "plt.style.use(\"default\")\n",
    "plt.imshow(img_data[5].max(axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad4a852-cd2c-49a5-9eb3-cba4306d75f3",
   "metadata": {},
   "source": [
    "Precomputed projections include max projects, mean projects, median projects, and center slices along z, y and x for all channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ec12e2-097f-41bc-a58c-b19f4b105d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "projections = [\n",
    "    'max_projection_x',\n",
    "    'max_projection_y',\n",
    "    'max_projection_z',\n",
    "    'mean_projection_x',\n",
    "    'mean_projection_y',\n",
    "    'mean_projection_z',\n",
    "    'median_projection_x',\n",
    "    'median_projection_y',\n",
    "    'median_projection_z',\n",
    "    'center_slice'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fd46e6-6d19-42ba-a565-a59a3d054859",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "f, axs = plt.subplots(len(projections), 7, figsize=(30, 30))\n",
    "for proj_ix, projection in enumerate(projections):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        proj = AICSImage(some_cell[projection]).data.squeeze()\n",
    "    \n",
    "    for ix, channel in enumerate(full_img.channel_names):\n",
    "        axs[proj_ix, ix].imshow(proj[ix])\n",
    "        \n",
    "        if proj_ix == 0:\n",
    "            axs[proj_ix, ix].set_title(channel, size=\"large\")\n",
    "        axs[proj_ix, ix].set_xticks([])\n",
    "        axs[proj_ix, ix].set_yticks([])\n",
    "\n",
    "\n",
    "    axs[proj_ix, 0].set_ylabel(projection, rotation=90, size='large')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4f5d25-e142-4725-ae1c-04cf9f0e17d9",
   "metadata": {},
   "source": [
    "To see all 3 projections (XY, YZ, and XZ) in a single image, we can use the **imgtoprojection** function to compute an \"all_projection\" image. Code associated with this function is available at https://github.com/AllenCellModeling/aicsimageprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b641d135-9701-4bf1-a17f-e991e865be0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets sample a TJP1 (Tight junction protein) cell in anaphase.\n",
    "# Cell cycle annotations are - \n",
    "# M0 - interphase\n",
    "# M1M2 - prophase\n",
    "# M3 - prometaphase\n",
    "# M4/M5 - metaphase\n",
    "# M6/M7_single - anaphase with only 1 of the daughters segmented\n",
    "# M6/M7_complete - anaphase with both daughters segmented\n",
    "\n",
    "df_mito = df.loc[df['cell_stage'].isin(['M6M7_complete'])]\n",
    "some_cell = df_mito.loc[df_mito['gene'] == 'TJP1'].sample(1).iloc[0]\n",
    "full_img = read_ome_zarr(some_cell[\"3d_image\"])\n",
    "\n",
    "# This will set:\n",
    "# DNA to Cyan\n",
    "# Membrane to Magenta\n",
    "# Structure to White\n",
    "\n",
    "colors = {'Segmentations': [[0,1,1], [1,1,1]],  # Lets only plot DNA and Structure for segs\n",
    "          'Raw intensities': [[0,1,1], [1,0,1], [1,1,1]]\n",
    "         }\n",
    "\n",
    "\n",
    "# Channels 1,2,3 are dna, mem, structure intensities\n",
    "# Channels 4,5,6 are dna, mem, structure segmentations\n",
    "channels = {'Segmentations': [4,6], 'Raw intensities': [1,2,3], }\n",
    "fig, axes = plt.subplots(1,2,figsize = (15, 5))\n",
    "\n",
    "for ind, (key, chan) in enumerate(channels.items()):\n",
    "    img_data = full_img.get_image_data(\"CZYX\", C=chan)\n",
    "    chan_names = [full_img.channel_names[i] for i in chan] \n",
    "    img_data = rescale_image(img_data, chan_names)\n",
    "    # Get all axes projection image\n",
    "    all_proj = imgtoprojection(\n",
    "        img_data,\n",
    "        proj_all=True,\n",
    "        proj_method=\"max\",\n",
    "        local_adjust=False,\n",
    "        global_adjust=True,\n",
    "        colors=colors[key],\n",
    "    )\n",
    "    \n",
    "    # Convert to YXC for PNG writing\n",
    "    all_proj = transforms.transpose_to_dims(all_proj, \"CYX\", \"YXC\")\n",
    "\n",
    "    # Drop size to uint8\n",
    "    all_proj = all_proj.astype(np.uint8)\n",
    "    axes[ind].imshow(all_proj)\n",
    "    axes[ind].set_title(key)\n",
    "    axes[ind].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5abbe2e-3b45-4acb-aa31-17c6a2a48082",
   "metadata": {},
   "source": [
    "Here, we have plotted all 3 projections of the raw intensity channels on the left, and segmentation channels on the right. For each subplot, the top right is the XY projection, the top left is the XZ projection, and the bottom is the YZ projection. The legend for this figure is shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df732758-f846-4ddd-a8c8-1cd5cb8ab380",
   "metadata": {},
   "source": [
    "<img src=\"resources/allproj_legend.png\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2611e412-3037-465b-86d1-0bffeaa647ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using diagnostic sheets to visualize many images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d479a-543f-4907-9bdd-5cc3d37effee",
   "metadata": {},
   "source": [
    "To visualize many cells that have been stratifed a certain way in a diagnostic sheet, we can use the **diagnostic_sheet** function. Code associated with this function is available at https://github.com/AllenCellModeling/aicsimageprocessing. For example, lets say we want to inspect images for edge cells vs non edge cells for differences in height and other aspects of shape. We can do so visually by generating diagnostic sheets for edge vs non edge cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51289755-da20-43c0-85d3-cae500c06136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets sample 20 cells of non edge (0) and edge (1) cells\n",
    "\n",
    "df_sample = df.loc[df['gene'].isin(['TJP1'])]\n",
    "df_sample = df_sample.groupby('edge_flag').apply(lambda x: x.sample(20)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10206a9d-accc-4d23-ad2a-ad22ea9c6728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets make 2 diagnostic sheets for non edge and edge cells \n",
    "# Here we can also specify a feature to be added as text on the plot\n",
    "# In this example, we plot nuclear height (column name MEM_position_depth)\n",
    "\n",
    "# This may take some time depending on the number of images being projected\n",
    "plt.style.use(\"dark_background\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "diagnostic_sheet(df_sample, \n",
    "                 save_dir = \"./\", # Created a diagnostic_sheets folder in the current working dir\n",
    "                 image_column = \"3d_image\", # Pass in the 3D image path or one of the 2D image paths like max_projection_x\n",
    "                 max_cells=25, # max cells per sheet\n",
    "                 channels = [1,2,3], # DNA, Membrane, Structure intensity channels\n",
    "                 colors = [[0, 1, 1], [1, 0, 1], [1,1,1]], # Cyan, Magenta, White\n",
    "                 proj_method = \"max\", # options - max, mean, sum\n",
    "                 metadata = \"edge_flag\", # Optional, Metadata to stratify the diagnostic sheets\n",
    "                 feature = \"cell_height\", # Optional, Feature to add as text,\n",
    "                 fig_width = None, # Default is number of columns * 7\n",
    "                 fig_height = None, # Default is number of rows * 5,\n",
    "                 distributed_executor_address = None, # An optional executor address to pass to some computation engine.\n",
    "                 batch_size = None, # process all at once\n",
    "                 overwrite=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a85e1d-f336-4038-b1de-410a550b03a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets load up the saved diagnostic sheets and compare edge and non edge cells\n",
    "# For a more zoomed in view, click on the image directly and view it in the browser\n",
    "fig, (ax, ax1) = plt.subplots(1,2,figsize=(20,20))\n",
    "ax.imshow(plt.imread(os.getcwd() + \"/diagnostic_sheets/\" + \"edge_flag_0_1.png\"))\n",
    "ax1.imshow(plt.imread(os.getcwd() + \"/diagnostic_sheets/\" + \"edge_flag_1_1.png\"))\n",
    "ax.set_axis_off()\n",
    "ax.set_title('Non edge cells')\n",
    "ax1.set_axis_off()\n",
    "ax1.set_title('Edge cells')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd25ec7-c00b-497d-9055-54ccb73de28e",
   "metadata": {},
   "source": [
    "*We can see visually that edge cells are generally taller than center cells. We can compute the average height per class and confirm the same. We can also see from the diagnostic sheet that edge cells are\n",
    "more tilted*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd86010-c0ba-4c8d-84c4-e0c97c17785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see here that non-edge cells are flatter than edge cells\n",
    "df.groupby('edge_flag').mean()['cell_height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c515f2-b2fc-4be0-b4f6-68ae7405ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can remove the diagnostic sheet directory if you want a clean directory\n",
    "# import shutil\n",
    "# import os\n",
    "# if os.path.isdir(os.getcwd() + \"/diagnostic_sheets/\"):\n",
    "#     shutil.rmtree(os.getcwd() + \"/diagnostic_sheets/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d5f23a-2bd6-4629-8138-6aec71c24b31",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <a id='end'></a> Conclusion\n",
    "In this chapter you have learned how the single cell images were preprocessed and how to visualize them in a few different ways. This has prepared you for some of the statistical anlaysis that we will show in the next Chapter 4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
