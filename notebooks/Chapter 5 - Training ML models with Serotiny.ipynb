{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26bed1c8-fc44-4369-b188-4170a5b9e017",
   "metadata": {},
   "source": [
    "Chapter 5: Training ML models with Serotiny\n",
    "Suggested: Alex, Gui\n",
    "- Quick explanation of serotinyâ€™s yaml-based task formulation\n",
    "- Show how one can start a simple training based on 2D images to classify, e.g. edge vs. non-edge cells\n",
    "- Show how one can load and apply the trained model\n",
    "- Show how to bring in a pretrained model (2D RESNET)\n",
    "- Show that we can use the latent space from 3D images (which has been precomputed and stored)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43473233-c3d4-4d95-be6f-bc93c511fcdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nbvv\n",
    "from upath import UPath as Path\n",
    "from aicsimageio import AICSImage\n",
    "from ome_zarr.reader import Reader\n",
    "from ome_zarr.io import parse_url\n",
    "import logging\n",
    "logging.getLogger(\"bfio\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"aicsimageio\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "# Should these functions not be \n",
    "\n",
    "def read_ome_zarr(path, level=0, image_name=\"default\"):\n",
    "    path = str(path if image_name is None else Path(path) / image_name)\n",
    "    reader = Reader(parse_url(path))\n",
    "\n",
    "    node = next(iter(reader()))\n",
    "    pps = node.metadata[\"coordinateTransformations\"][0][0][\"scale\"][-3:]\n",
    "   \n",
    "    return AICSImage(\n",
    "        node.data[level].compute(),\n",
    "        channel_names=node.metadata[\"name\"],\n",
    "        physical_pixel_sizes=pps\n",
    "    )\n",
    "\n",
    "def rescale_image(img_data, channels):\n",
    "    img_data = img_data.squeeze().astype(np.float32)\n",
    "    \n",
    "    for ix, channel in enumerate(channels):\n",
    "        if \"_seg\" not in channel:\n",
    "            img_data[ix] -= 1\n",
    "            \n",
    "            img_data[ix] = np.where(\n",
    "                img_data[ix] >= 0,\n",
    "                img_data[ix] / img_data.max(),\n",
    "                -1\n",
    "            )\n",
    "    return img_data.astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f337331-424d-4139-b2e8-06958c139950",
   "metadata": {},
   "source": [
    "## Load the manifest and explore dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e619ed29-e90a-4669-b212-302b75a4371c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cells: 215081\n",
      "Number of columns: 1242\n"
     ]
    }
   ],
   "source": [
    "cells_df = pd.read_parquet(\"s3://variance-dataset/processed/manifest.parquet\")\n",
    "print(f'Number of cells: {len(cells_df)}')\n",
    "print(f'Number of columns: {len(cells_df.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96afa7-d31c-4f67-9f76-0736439342eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Make a simple data of edge vs. non-edge cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "137c8837-2b7c-408c-af79-bd2b12798e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3281/2312792999.py:7: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  index = pd.Series([])\n",
      "/tmp/ipykernel_3281/2312792999.py:9: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  index = index.append(\n",
      "/tmp/ipykernel_3281/2312792999.py:9: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  index = index.append(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cells: 2000\n",
      "Number of columns: 1243\n"
     ]
    }
   ],
   "source": [
    "from serotiny.transforms.dataframe.transforms import split_dataframe\n",
    "Path('./serotiny_data/').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "n = 1000 #number of cells per class\n",
    "# Sample cells for each class\n",
    "edge_label = cells_df[\"edge_flag\"].unique()\n",
    "index = pd.Series([])\n",
    "for s, struct in enumerate(edge_label):\n",
    "    index = index.append(\n",
    "        cells_df[cells_df[\"edge_flag\"] == s]\n",
    "        .sample(n=n)\n",
    "        .index.to_series()\n",
    "    )\n",
    "cells_edgeVSnoedge = cells_df.loc[index]\n",
    "# Add the train, test and validate split\n",
    "cells_edgeVSnoedge = split_dataframe(dataframe=cells_edgeVSnoedge,train_frac=0.7,val_frac=0.2,return_splits=False)\n",
    "#\n",
    "cells_edgeVSnoedge.to_csv('./serotiny_data/cells_edgeVSnoedge.csv') \n",
    "print(f'Number of cells: {len(cells_edgeVSnoedge)}')\n",
    "print(f'Number of columns: {len(cells_edgeVSnoedge.columns)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afb1e3b-400a-447f-a082-73a434164aaf",
   "metadata": {},
   "source": [
    "https://allencell.github.io/serotiny/getting_started.html\n",
    "Using the cookiecutter to create a serotiny project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf6dc508-63df-449a-8a79-d6012046d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cookiecutter | grep -v 'already satisfied' #avoid warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5aadbb-82d3-4c00-915e-3ec57cfa22b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cookiecutter https://github.com/AllenCellModeling/serotiny-project-cookiecutter ran this in the terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1177cb03-9a0c-4e18-bbd3-4cedf3b3af7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/aicsuser/cytodata-hackathon-base/notebooks/ch5_attempt1\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: ch5-attempt1\n",
      "  Building editable for ch5-attempt1 (pyproject.toml): started\n",
      "  Building editable for ch5-attempt1 (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for ch5-attempt1: filename=ch5_attempt1-0.0.0-py3-none-any.whl size=1405 sha256=4b7629a0c33122ea34b31f5b4d531bbde871f5fc00946050363d2bbf8634b237\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-0wa_uwbc/wheels/3e/62/f7/f13b52bb2a4764d3dc44abce459c034c4fdc2fd0a2658e66de\n",
      "Successfully built ch5-attempt1\n",
      "Installing collected packages: ch5-attempt1\n",
      "  Attempting uninstall: ch5-attempt1\n",
      "    Found existing installation: ch5-attempt1 0.0.0\n",
      "    Uninstalling ch5-attempt1-0.0.0:\n",
      "      Successfully uninstalled ch5-attempt1-0.0.0\n",
      "Successfully installed ch5-attempt1-0.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ch5_attempt1/ | grep -v 'already satisfied' #avoid warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed63cfdb-e71e-4883-8547-074ca85b7f72",
   "metadata": {},
   "source": [
    "### Show image info using serotiny CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e112ae66-d284-4ab1-8a2f-b2d34efbebe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted file (variance-dataset/max_projection_z/408295.ome.tiff) load with reader: aicsimageio.readers.bfio_reader.OmeTiledTiffReader failed with error: No module named 'bfio'\n",
      "/opt/conda/lib/python3.9/site-packages/ome_types/_convenience.py:105: FutureWarning: The default XML parser will be changing from 'xmlschema' to 'lxml' in version 0.4.0.  To silence this warning, please provide the `parser` argument, specifying either 'lxml' (to opt into the new behavior), or'xmlschema' (to retain the old behavior).\n",
      "  d = to_dict(os.fspath(xml), parser=parser, validate=validate)\n",
      "Image shape:  (7, 245, 381)\n",
      "Channel names:  ['bf', 'dna', 'membrane', 'structure', 'dna_segmentation', 'membrane_segmentation', 'struct_segmentation_roof']\n"
     ]
    }
   ],
   "source": [
    "!serotiny image info s3://variance-dataset/max_projection_z/408295.ome.tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6191d211-8dc4-496b-8e1a-36c5b4062491",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/serotiny\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/serotiny/cli/cli.py\", line 58, in main\n",
      "    hydra.main(config_path=None, config_name=mode, version_base=None)(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/main.py\", line 90, in decorated_main\n",
      "    _run_hydra(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/utils.py\", line 389, in _run_hydra\n",
      "    _run_app(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/utils.py\", line 452, in _run_app\n",
      "    run_and_report(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/utils.py\", line 216, in run_and_report\n",
      "    raise ex\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/utils.py\", line 213, in run_and_report\n",
      "    return func()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/utils.py\", line 453, in <lambda>\n",
      "    lambda: hydra.run(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/hydra.py\", line 105, in run\n",
      "    cfg = self.compose_config(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/hydra.py\", line 594, in compose_config\n",
      "    cfg = self.config_loader.load_configuration(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/config_loader_impl.py\", line 141, in load_configuration\n",
      "    return self._load_configuration_impl(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/config_loader_impl.py\", line 242, in _load_configuration_impl\n",
      "    defaults_list = create_defaults_list(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/defaults_list.py\", line 750, in create_defaults_list\n",
      "    defaults, tree = _create_defaults_list(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/defaults_list.py\", line 720, in _create_defaults_list\n",
      "    defaults_tree = _create_defaults_tree(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/defaults_list.py\", line 356, in _create_defaults_tree\n",
      "    ret = _create_defaults_tree_impl(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/defaults_list.py\", line 457, in _create_defaults_tree_impl\n",
      "    return _expand_virtual_root(repo, root, overrides, skip_missing)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/defaults_list.py\", line 280, in _expand_virtual_root\n",
      "    subtree = _create_defaults_tree_impl(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/defaults_list.py\", line 577, in _create_defaults_tree_impl\n",
      "    add_child(children, new_root)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/defaults_list.py\", line 520, in add_child\n",
      "    subtree_ = _create_defaults_tree_impl(\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/defaults_list.py\", line 466, in _create_defaults_tree_impl\n",
      "    update_package_header(repo=repo, node=parent)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/defaults_list.py\", line 262, in update_package_header\n",
      "    loaded = repo.load_config(config_path=node.get_config_path())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/config_repository.py\", line 349, in load_config\n",
      "    ret = self.delegate.load_config(config_path=config_path)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/config_repository.py\", line 92, in load_config\n",
      "    ret = source.load_config(config_path=config_path)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/hydra/_internal/core_plugins/file_config_source.py\", line 31, in load_config\n",
      "    cfg = OmegaConf.load(f)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/omegaconf/omegaconf.py\", line 190, in load\n",
      "    obj = yaml.load(file_, Loader=get_yaml_loader())\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/yaml/__init__.py\", line 81, in load\n",
      "    return loader.get_single_data()\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/yaml/constructor.py\", line 51, in get_single_data\n",
      "    return self.construct_document(node)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/yaml/constructor.py\", line 60, in construct_document\n",
      "    for dummy in generator:\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/yaml/constructor.py\", line 413, in construct_yaml_map\n",
      "    value = self.construct_mapping(node)\n",
      "  File \"/opt/conda/lib/python3.9/site-packages/omegaconf/_utils.py\", line 134, in construct_mapping\n",
      "    raise yaml.constructor.ConstructorError(\n",
      "yaml.constructor.ConstructorError: while constructing a mapping\n",
      "  in \"/home/aicsuser/work/cytodata-hackathon-base/notebooks/ch5_attempt1/ch5_attempt1/config/model/class2d_model.yaml\", line 2, column 3\n",
      "found duplicate key _\n",
      "  in \"/home/aicsuser/work/cytodata-hackathon-base/notebooks/ch5_attempt1/ch5_attempt1/config/model/class2d_model.yaml\", line 3, column 3\n"
     ]
    }
   ],
   "source": [
    "!cd ch5_attempt1/;HYDRA_FULL_ERROR=1 serotiny train model=class2d_model data=class2d_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2f40399-7b4f-4892-a985-fb186f87e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b5119e5-cc80-4c66-bf15-27df43e3cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra.utils import instantiate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95fd1097-f62c-4eca-beaf-9ca7f697c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = instantiate(yaml.full_load('''_target_: serotiny.datamodules.ManifestDatamodule\n",
    "\n",
    "path: serotiny_data/cells_edgeVSnoedge.csv\n",
    "\n",
    "batch_size: 64\n",
    "num_workers: 1\n",
    "loaders:\n",
    "  id:\n",
    "    _target_: serotiny.io.dataframe.loaders.LoadColumn\n",
    "    column: CellId\n",
    "    dtype: int\n",
    "  class:\n",
    "    _target_: serotiny.io.dataframe.loaders.LoadColumn\n",
    "    column: edge_flag\n",
    "    dtype: int\n",
    "  image:\n",
    "    _target_: serotiny.io.dataframe.loaders.LoadImage\n",
    "    column: max_projection_z\n",
    "    select_channels: ['membrane']\n",
    "split_column: \"split\"'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d6832d7-b89a-4bc4-b09a-2abe69044e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/aicsuser/work/cytodata-hackathon-base/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e920a095-3453-44ad-afae-3c234d5f58c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dl = data.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3301050-ff87-446e-8098-bb1c90ed8a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f9afa5d7190>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db904789-b3b0-40a6-bc3e-5008cb47f333",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_dl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a92178f-b094-4160-b077-98914982d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41858fbb-d82e-4611-82c1-203ba6bce642",
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "152d0223-cdcb-4d26-91b4-1697fdefb93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = instantiate(yaml.full_load(''' \n",
    "_aux_: \n",
    "  _: &hidden_channels 4\n",
    "  _: &kernel_size 3\n",
    "  _: &conv_block\n",
    "    _target_: torch.nn.Sequential\n",
    "    _args_:\n",
    "      - _target_: torch.nn.LazyConv3d\n",
    "        out_channels: *hidden_channels\n",
    "        kernel_size: *kernel_size\n",
    "        stride: 1\n",
    "      - _target_: torch.nn.LeakyReLU\n",
    "      - _target_: torch.nn.LazyBatchNorm3d\n",
    "_target_: ch5_attempt1.ch5_attempt1.model.Classifier\n",
    "x_label: image\n",
    "y_label: class\n",
    "network:\n",
    "  _target_: torch.nn.Sequential\n",
    "  _args_:\n",
    "    - *conv_block\n",
    "    - *conv_block\n",
    "    - *conv_block\n",
    "loss:\n",
    "  _target_: torch.nn.CrossEntropyLoss'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c5c963a-035b-4632-ae54-60c605cd7438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (network): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): LazyConv3d(0, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): LazyBatchNorm3d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): LazyConv3d(0, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): LazyBatchNorm3d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): LazyConv3d(0, 4, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): LazyBatchNorm3d(0, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f140905-cbf4-4621-8883-2ae03ba851c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ch5_attempt1.ch5_attempt1.model import Classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
